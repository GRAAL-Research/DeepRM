cff-version: 1.2.0
title: >-
  Software for the article "Generalization Bounds via
  Meta-Learned Model Representations: PAC-Bayes and Sample
  Compression Hypernetworks"
message: >-
  message: "If you use this software, please cite it as
  below."
type: software
authors:
  - given-names: Benjamin
    family-names: Leblanc
    affiliation: Laval University
    email: benjamin.leblanc.2@ulaval.ca
  - given-names: Mathieu
    family-names: Bazinet
    affiliation: Laval University
    email: mathieu.bazinet.2@ulaval.ca
  - given-names: Nathaniel
    family-names: D'Amours
    affiliation: Laval University
    email: nathaniel.damours.1@ulaval.ca
  - given-names: Alexandre
    family-names: Drouin
    affiliation: 'ServiceNow Research, Laval University'
    email: alexandre.drouin@servicenow.com
  - given-names: Pascal
    family-names: Germain
    affiliation: Laval University
preferred-citation:
  type: article
  authors:
  - given-names: Benjamin
    family-names: Leblanc
    affiliation: Laval University
    email: benjamin.leblanc.2@ulaval.ca
  - given-names: Mathieu
    family-names: Bazinet
    affiliation: Laval University
    email: mathieu.bazinet.2@ulaval.ca
  - given-names: Nathaniel
    family-names: D'Amours
    affiliation: Laval University
    email: nathaniel.damours.1@ulaval.ca
  - given-names: Alexandre
    family-names: Drouin
    affiliation: 'ServiceNow Research, Laval University'
    email: alexandre.drouin@servicenow.com
  - given-names: Pascal
    family-names: Germain
    affiliation: Laval University
  doi: "10.48550/arXiv.2410.13577"
  journal: "ICML 2025"
  title: "Generalization Bounds via Meta-Learned Model Representations: PAC-Bayes and Sample Compression Hypernetworks"
  abstract: "Both PAC-Bayesian and Sample Compress learning frameworks are instrumental for deriving tight (non-vacuous) generalization bounds for neural networks. We leverage these results in a meta-learning scheme, relying on a hypernetwork that outputs the parameters of a downstream predictor from a dataset input. The originality of our approach lies in the investigated hypernetwork architectures that encode the dataset before decoding the parameters: (1) a PAC-Bayesian encoder that expresses a posterior distribution over a latent space, (2) a Sample Compress encoder that selects a small sample of the dataset input along with a message from a discrete set, and (3) a hybrid between both approaches motivated by a new Sample Compress theorem handling continuous messages. The latter theorem exploits the pivotal information transiting at the encoder-decoder junction in order to compute generalization guarantees for each downstream predictor obtained by our meta-learning scheme. "
  url: "https://github.com/GRAAL-Research/DeepRM"